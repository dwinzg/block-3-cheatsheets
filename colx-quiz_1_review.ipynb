{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575def13",
   "metadata": {},
   "source": [
    "* How could you use slicing to implement strip()?  Why wouldn't you want to?\n",
    "\n",
    "Ans: It is doable using slicing to achieve the same effect of strip() by slicing the spaces out and keep the strs. However, it is difficult to count which position that the space ends or starts, so we will want to use strip() to do it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3b516d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quick Fox 192'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '     Quick Fox 192,...../    '\n",
    "s.strip()\n",
    "s[5:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549871ba",
   "metadata": {},
   "source": [
    "* How could we build a quick number-checker that captures all floats (ie, negatives and decimals)?  (No Regexes or pre-built functions!).\n",
    "\n",
    "Ans: We can build a quick number-checker by checking each character in the string to see if it is a digit, a decimal point, or a negative sign. We can also ensure that there is at most one decimal point and that the negative sign is only at the beginning of the string. If all characters meet these criteria, we can consider it a valid float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706bf886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumberChecker(number):\n",
    "    for i in number:\n",
    "        if i not in '0123456789.-':\n",
    "            return False\n",
    "    if number.count('.') > 1:\n",
    "        return False\n",
    "    if number.count('-') > 1 or (number.count('-') == 1 and number[0] != '-'):\n",
    "        return False\n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca2fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumberChecker('-12.34')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18398b",
   "metadata": {},
   "source": [
    "* Why might you want to preserve case when analyzing proper nouns for translation?\n",
    "\n",
    "Ans: Some of nouns are capitalized, and if we do not preserve the case, we might misinterpret the meaning of the nouns. For example, \"Apple\" (the company) and \"apple\" (the fruit) have different meanings, so preserving case helps maintain the correct context for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb18e6",
   "metadata": {},
   "source": [
    "* How can .split() be used to quickly count how many lines of text are in a document?\n",
    "\n",
    "Ans: We can use .split('\\n') to split the text into a list of lines based on the newline character. The length of this list will give us the total number of lines in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abc8e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is line one.\n",
      "This is line two.\n",
      "This is line three.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This is line one.', 'This is line two.', 'This is line three.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is line one.\\nThis is line two.\\nThis is line three.'\n",
    "print(text)\n",
    "lines = text.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f172c",
   "metadata": {},
   "source": [
    "* How would you find all the consonants in a string using a single string function, using the string \"aeiou\" (refrain from using functions not discussed in class)?\n",
    "\n",
    "Ans: We can use the .replace() function to remove all the vowels from the string, leaving only the consonants. For example, we can iterate through each vowel in \"aeiou\" and replace it with an empty string in the original string. After processing all vowels, the remaining characters will be the consonants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b454bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindConsonants(s):\n",
    "    [i for i in s if i.lower() not in 'aeiou' and i.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fdd14b",
   "metadata": {},
   "source": [
    "* Write a function that capitalizes the first letter of each word that is longer than 3 characters, without using .title() or external libraries. What assumptions do you need to make about the input?\n",
    "\n",
    "Ans: I need to make that the word only contains letters, no comma, digit point, or any other symbol that is not letter. Furthermore, I need to assume that the word is not capitalized. Based on these two assumptions, I can match the first character of the word with if statement and replace the first letter in to capitalized form using slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a62ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Capitalization(w):\n",
    "    if w > 3 and w[0]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b12a0",
   "metadata": {},
   "source": [
    "* Write a function that removes any words that contain numeric characters from a sentence (again, no Regexes).  It should return the input sentence with such words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f7ae536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  988 foxe3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'I ha3e 988 foxe3'.replace('ha3e', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "590efade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNumeric(s):\n",
    "    output = []\n",
    "    for word in s.split(' '):\n",
    "        if word.isdigit() == False or word.isnumeric() == False:\n",
    "            output.append(word)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b447b1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'around', 'foxe3,', 'it\"s']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RemoveNumeric('I have around 988 foxe3, it\"s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a56608",
   "metadata": {},
   "source": [
    "* Write a function that checks whether a string is a valid email address (e.g., \"user@example.com\").  The function should only use functions discussed in class. What kinds of invalid inputs should your function catch? Are there borderline cases you might allow? How would you handle them? Write 3 test cases — 2 valid, 1 invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4ef09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_email(email):\n",
    "    # 1. Must be a string with no spaces\n",
    "    if not isinstance(email, str):\n",
    "        return False\n",
    "    if \" \" in email:\n",
    "        return False\n",
    "\n",
    "    # 2. Must contain exactly one '@'\n",
    "    if email.count(\"@\") != 1:\n",
    "        return False\n",
    "\n",
    "    local, domain = email.split(\"@\")\n",
    "\n",
    "    # 3. Local and domain parts must be non-empty\n",
    "    if len(local) == 0 or len(domain) == 0:\n",
    "        return False\n",
    "\n",
    "    # 4. Domain must contain at least one dot, with text on both sides\n",
    "    if \".\" not in domain:\n",
    "        return False\n",
    "\n",
    "    # e.g. domain = \"example.com\" -> [\"example\", \"com\"]\n",
    "    parts = domain.split(\".\")\n",
    "    # No empty parts like \"example.\" or \".com\" or \"example..com\"\n",
    "    for p in parts:\n",
    "        if len(p) == 0:\n",
    "            return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43447192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "assert is_valid_email(\"alice.smith23@cs.ubc.ca\") == True\n",
    "assert is_valid_email(\"user+tag@example.com\") == True\n",
    "assert is_valid_email(\"no-at-symbol.example.com\") == False\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22d3f0",
   "metadata": {},
   "source": [
    "* Which sections of the Zipfian curve are likely to differ the most across corpora (with respect to which words occur where)?  Briefly explain.\n",
    "\n",
    "Ans: The stop word section will be the most different across the corpora because stopwords occurs much more frequently compare to other word types, hence most of the stop words will occurs at the top part of the Zipfian curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8e55e",
   "metadata": {},
   "source": [
    "* If a language has a highly synthetic morphology (many affixes), would you expect it to have a higher or lower Type-Token Ratio (TTR) than a language with less rich morphological structure? Briefly explain why.\n",
    "\n",
    "Ans: Type-Token Ratio is measured by the word types divided by the number of tokens, hence if a language has highly synthetic morphology that some types of words are composed by some affixes, then the number of tokens will be larger compare to those languages that are not highly synthetic morphology. Hence, the TTR will be lower which means that the language reuse words multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d71302",
   "metadata": {},
   "source": [
    "* What role does linguistic annotation provide for corpora, specifically for computational linguistics?\n",
    "\n",
    "Ans: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d9830",
   "metadata": {},
   "source": [
    "* As we increase the size of a corpus, the frequency of Hapax Legomena generally increases. Would the frequency of function words like \"the\" or \"is\" also increase? Why or why not?\n",
    "\n",
    "Ans: No, Hapax Legomena will only count each unique word once. So even though increase the size of a corpus, the frequency of function words will not increase if these function words already been recorded by the Hapax Legomena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc6b25",
   "metadata": {},
   "source": [
    "* Why is it important to understand the intended audience and time period of a corpus when conducting linguistic analysis?\n",
    "\n",
    "Ans: In no matter which statistical analysis, we care about our population. If we are using data that not correlated with the interested target, our analysis is not valid in that population/target. For example, if we want to study for the Standard Mandarin, we can not analyze a Mandarin Dialect because the data is not matching with our target, similarly, we can not analyze how Standard Mandarin in 1800 because Mandarin had changed a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f17ff",
   "metadata": {},
   "source": [
    "* If you were to analyze a corpus for stylistic differences, how might you determine: the formality of the language; whether it’s written or spoken; its sentiment? Assume that we don't have existing ML tools or enough data to train one.\n",
    "\n",
    "Ans: Without giving any other information, I have to assume that I understand the language that the corpus is composed. To analyze the formality of the language, I will count the frequency of informal expressions or abbrevatives such as kinda, yeah, don't, and other kinds of slangs, meanwhile, I can navigate the word complexity of the corpus, the more complexy and less information frequency will indicate that the corpus is formal and vice versa. Furthermore, for determining whether it's written or spoken, I can navigate whether the corpus contains oral expressions such as gonna, wanna, yeah, should, and their corresponding frequencies. For Sentimental analyze, I can mannually create a dictionary that contains the positive words and negative words I known in the language, then iterate over the corpus and count the frequency of those words to see if one's overly large than another, otherwise the corpus will be neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca5a58",
   "metadata": {},
   "source": [
    "* If you observe that a particular word has high frequency but extremely uneven distribution across corpora, what might this indicate about its function or meaning?\n",
    "\n",
    "Ans: Based on the information, I would like to say the word is a specific word for a subsection of the whole corpora. For example, I have a Science Corpora, and inside of it has a Data Science subsection, in this section, the word: pandas will have higher frequency compare to other subsections. Meanwhile, it also may represent the changes on language through time if we are analyzing a corpora from 1950 to 2020, there won't be any thing called Tiktok in 1950-2010, but this word has high frequency from 2010-2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2a90a",
   "metadata": {},
   "source": [
    "* Imagine you are working with a corpus in a language you don't know, and you need to identify high-frequency content words in it (let's call them \"salient tokens\"). You cannot use machine learning but can perform basic statistical analysis. How would you approach identifying these words? What metrics would help you confirm that you've identified them correctly (or is that even possible)?\n",
    "\n",
    "Ans: In order to do so, I will draw a Zipf's Curve for the corpus, and remove the most frequent words shown in the Zipf's curve, based on the defination of Zipfian Curve, these words are usually function words or stopwords that does not contain any infomration. However, we still can not guarente that the word is a content word, hence, we can analyze how the word is distributed among the corpus, if the word is evenly distributed, then it might be a stopword, but if it somehow clustered at a certain range, then it will be a content word. At last, we can use tfidf to identify the content words among different corpus if the corpus has labled it and we have a different corpus in the same language but in different category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b6128",
   "metadata": {},
   "source": [
    "* How does a defaultdict differ from a regular dictionary in Python? (2 differences)\n",
    "\n",
    "Ans: \n",
    "1. If we try to access item that are not in the list of keys, the regular dictionary will throw an keyerror to us, to solve it, we have to use dict.get('key', value_if_not_found) to access the dictionary safely. But in defaultdict, it will return the default value when creating the defaultdict if the key is missing, such 0.0 if defaultdict(float) or '' if defaultdict(str). \n",
    "\n",
    "2. Regular dictionary can not automatically create a data type if we don'e specifically asign it, such as a list or a float. But in defaultdict, we can specify the data type we want during create the defaultdict, for example: defaultdict(list), so that all the values we pass in or automatically created will be stored in a form of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f6041",
   "metadata": {},
   "source": [
    "* Why do we not care about the extra space required to create a reverse index? (2 reasons)\n",
    "\n",
    "Ans: \n",
    "1. We are trading space complexity with time complexity, the time complexity if we want to retrive the key based on value will be O(n) if not creating reverse index, but O(1) after we create a reverse index.\n",
    "2. The space that taken by the reverse index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953baa7",
   "metadata": {},
   "source": [
    "* When would we want to represent linguistic data in a list, instead of a dictionary or a set?\n",
    "\n",
    "Ans: Dictionary and a set do not preserve the order of we recording linguistic data, so if we want to preserve the order of the data like we are recording a sentence or POS tags, we need to use list. Secondly, dictionary or a set can not contain duplicate elements, so if we want to preserve the duplicate elements, for example, we want to count the frequencies of the words, we need to use a list instead of a dictionary or a set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f60d08",
   "metadata": {},
   "source": [
    "* When we nest deep structures in dictionaries, we lose their O(1) benefits.  Can you think of a better way to represent complex data sets?\n",
    "\n",
    "Ans: Instead of having list inside dictionaries, we can flatten the structure by adding more attributes to the key such as document id, sentence_id, token_index, then we add the value into the key. This allow us to keep the O(1) time complexity through the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22dbfe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_sense_lexicon = {\"bear\":[{\"POS\":\"noun\",\"animate\":True,\"count\":634,\"gloss\":\"A big furry animal\"},\n",
    "                              {\"POS\":\"verb\",\"transitive\":True,\"count\":294, \"past tense\":\"bore\", \"past participle\":\"borne\", \"gloss\":\"to endure\"}],\n",
    "                      \"slug\":[{\"POS\":\"noun\",\"animate\":True,\"count\":34, \"gloss\":\"A slimy animal\"},\n",
    "                              {\"POS\":\"verb\",\"transitive\":True,\"count\":3, \"gloss\": \"to hit\"}],\n",
    "                      \"back\":[{\"POS\":\"noun\",\"animate\":False,\"count\":12,\"gloss\":\"a body part\"},\n",
    "                              {\"POS\":\"noun\",\"animate\":False,\"count\":43, \"gloss\":\"the rear of a place\"},\n",
    "                              {\"POS\":\"verb\",\"transitive\":True,\"count\":5, \"gloss\":\"to support\"},\n",
    "                              {\"POS\":\"adverb\",\"count\":47,\"gloss\":\"in a returning fashion\"}],\n",
    "                      \"good\":[{\"POS\":\"noun\",\"animate\":False,\"count\":19,\"gloss\":\"a thing of value\"},\n",
    "                              {\"POS\":\"adjective\", \"count\":1293,\"gloss\":\"positive\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86a39616",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {\n",
    "    (\"bear\", \"noun\", 1): {\"animate\": True, \"count\": 634,\n",
    "                          \"gloss\": \"A big furry animal\"},\n",
    "\n",
    "    (\"bear\", \"verb\", 1): {\"transitive\": True, \"count\": 294,\n",
    "                          \"past tense\": \"bore\", \"past participle\": \"borne\",\n",
    "                          \"gloss\": \"to endure\"},\n",
    "\n",
    "    (\"slug\", \"noun\", 1): {\"animate\": True, \"count\": 34,\n",
    "                          \"gloss\": \"A slimy animal\"},\n",
    "\n",
    "    (\"slug\", \"verb\", 1): {\"transitive\": True, \"count\": 3,\n",
    "                          \"gloss\": \"to hit\"},\n",
    "\n",
    "    (\"back\", \"noun\", 1): {\"animate\": False, \"count\": 12,\n",
    "                          \"gloss\": \"a body part\"},\n",
    "    (\"back\", \"noun\", 2): {\"animate\": False, \"count\": 43,\n",
    "                          \"gloss\": \"the rear of a place\"},\n",
    "    (\"back\", \"verb\", 1): {\"transitive\": True, \"count\": 5,\n",
    "                          \"gloss\": \"to support\"},\n",
    "    (\"back\", \"adverb\", 1): {\"count\": 47,\n",
    "                            \"gloss\": \"in a returning fashion\"},\n",
    "\n",
    "    (\"good\", \"noun\", 1): {\"animate\": False, \"count\": 19,\n",
    "                          \"gloss\": \"a thing of value\"},\n",
    "    (\"good\", \"adjective\", 1): {\"count\": 1293, \"gloss\": \"positive\"}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7c966",
   "metadata": {},
   "source": [
    "* What is the Big O time complexity of finding the elements in a set that intersect with an iterable (ie, string, list, etc)?  Briefly explain.\n",
    "\n",
    "Ans: The time complexity of such an operation is O(n), where n is the length of the iterables we are intersecting. This is because a intersection check on a set is O(1), but we need to check through all the elements in an iterable object, so O(n) in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc66131",
   "metadata": {},
   "source": [
    "* Lexicons are useful for initial text analysis but often lack the adaptability needed for advanced NLP tasks. Why is this the case? Provide at least 2 reasons with brief explanations.\n",
    "\n",
    "Ans: \n",
    "1. Lexicon labels every word with a sentiment level of Positive, Negative, or Neutral, however, it is not the real word case becasue context may change the sentiment of a word. For example: Charge the phone, Criminal Charge, and take charge of something, these three phrases all use the word 'charge', but in three different sentiment level, where the lexicon will not be able to identify such a correlation.\n",
    "\n",
    "2. Lexicons can not handle the new word, slangs, sarcasm, or idioms. In the first place, lexicons can not adapt to new vocabularies such as iPhone if we are using a 1980 lexicon, or Tiktok if we are using 2010 Lexicon. Furthermore, it can not identify the salngs, sarcasm, or idioms that may having different meanings with the words, such as break a leg or any sarcasm phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d2175",
   "metadata": {},
   "source": [
    "* Imagine you have a large text corpus in English and Spanish and want to automatically align sentences for machine translation. What are some straightforward methods you could use to identify sentence pairs that are likely translations of each other?\n",
    "\n",
    "Ans: The first one I can think of is punctuations, the sentence that share the same puncuations or puncuations' position will be highly likely to be the same sentence. Secondly, we can identify them by mutal words, such as information / información, animal / animal, region / región, along with the punctuation identify, we can be more confident of the classification. Thirdly, we can use proper noun matching such as Country names, People Names, or any other proper noun matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdb6b3",
   "metadata": {},
   "source": [
    "* Imagine that you travel back to 1816, just as Franz Bopp begins formulating the comparative method (ie, where different languages are compared to identify related words, and ultimately, sound changes that suggest the languages are related). You bring a 21st-century computational toolkit, but due to temporal paradox laws, you are not allowed to bring any modern reconstructions of Proto-Indo-European (PIE) (We cannot reconstruct PIE from reconstructed PIE - the chicken needs an egg!).  Bopp has access only to the sources available in 1816: Sanskrit grammars, Greek and Latin dictionaries, scattered Gothic texts, early editions of Old Irish, and some Old Persian inscriptions.  If you could collect additional lexicons that would help confirm or deny his work, what information would be required?  That is, what additional information would you provide that could help him formulate his theories?  You should provide at least 5 sources that would be helpful, rank them in order of helpfulness, and briefly explain why it would be help discover related languages.  (You can assume that your laptop will magically work in 1816, Bopp won't be more interested in the computer than the information, and that there is no internet connection.)\n",
    "\n",
    "Ans: ??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c9062",
   "metadata": {},
   "source": [
    "* What are two potential drawbacks of removing stopwords from a text before conducting a machine translation?\n",
    "\n",
    "Ans: In some of the language like English, stopwords contain grammatical informaiton like the tense of the sentence or the role that each noun is playing in the sentence, removing the stopwords will confuse the model because it can not understand how to determine the tense or the role, which result in misinterpretation during the translation. Moreover, stopwords may explain the noun like 'the essay' or 'an essay', removing the stop word will trim the definition out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e6601",
   "metadata": {},
   "source": [
    "* Given a list of tuples where the first element is a string and the second is an integer, write a short piece of code to sort the list in descending order based on the second element. Briefly explain your approach.\n",
    "\n",
    "Ans: I will use temp function lambda to do it, the reason it is working because the lambda x indicates each tuple in the list, and we are sorting the list by x[1] which is the integer part of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c123d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_tuples = [('a', 1), ('b', 3), ('c', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "801826b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('c', 2), ('b', 3)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lst_of_tuples, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccded0ed",
   "metadata": {},
   "source": [
    "* Why does the lexical diversity (type-to-token ratio) typically increase when analyzing smaller sub-corpora rather than larger ones? What does this suggest about the content of smaller texts?\n",
    "\n",
    "Ans: Lexical diverstiy measuring how many different types of token are there in a corpora instead of the total count of a certain type. For a small sub-corpora, it is more likely the TTR to see unknown word type, while in larger corpora, most of the words will be repeated because the model already record the same word type before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885577",
   "metadata": {},
   "source": [
    "* Would you expect a higher or lower frequency of passive voice constructions in news documents compared to casual conversation? Briefly explain your reasoning. (Remember that passive voice is a structure like \"the tree was cut down\", inverting the subject and object).\n",
    "\n",
    "Ans: A higher frequency of passive voice will be expected because the news often uses the passive voice to report the event such as 'policy is being activated' or 'Some people are injured'. While in casual conversation, people will focus more on the people instead of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7241ec0",
   "metadata": {},
   "source": [
    "* In class, we removed stopwords by using a lexicon.  Can you think of another way that we could remove all closed class words?\n",
    "\n",
    "Ans: There are several way we can do so, the first one that I can think of is using Zipfian's Curve, where the top part of the curve will be the most frequent in each language and will highly likely to be a closed class word. Secondly, if we have more corpous in a same language, we can compare the proportion of each word with another corpora, for those with lower ratios will be the closed words because they appear evenly on both corpus. Thirdly, we can use tfidf to identify the closed words, which perform the same as method 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c034a",
   "metadata": {},
   "source": [
    "* You are analyzing a corpus to identify overused phrases or clichés (e.g., \"at the end of the day,\" \"time will tell\"). How might you identify phrases that the author has overused (without using a list of phrases)? To be clear, these are phrases that are overused in the document, not over all of language.\n",
    "\n",
    "Ans: I can think of two ways to identifying the overused phrases. First, we can use n-grams to identify the frequency of the phrase. For example, 'at the end of the day' is a 6-gram, which the corpus will not have a lot same phrase if the author is not abusing the phrase, if it appears more than a certain threshold, then the author is abusing it. Secondly, we can use tfidf within the document by spliting the corpus into different sections so that we can implement tfidf normally but within a single corpus, if a phrase has high tf and low idf, then it might indicates that the author is abusing the phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fca26a",
   "metadata": {},
   "source": [
    "* In written French, negation is often indicated by \"ne ... pas\" (ie, \"je ne parle pas\" - \"I am not speaking\"; \"tu ne conduis pas\" - \"You are not driving\", etc.).  However, in some French dialects, \"ne\" is often (but not always) dropped in speech: \"tu parles pas.\".  Making things more difficult, there are some phrases in French that use \"ne\", but not \"pas\", so it's not as simple as counting \"ne\" and \"pas\".  Using this information, how would you determine whether a corpus was composed of written or spoken French?  You don't need to write the code, but explain the logic that you would use to come to this conclusion.\n",
    "\n",
    "Ans: In the first place, I'm assuming that I don't understand French and I have another corpus of french in a form I already know is written or spoken, based on that, we can count the frequencies of 'ne' and 'pas' in the corpus. For example, I have a corpus that I know is in written form with about the same size as the new corpus, and the new corpus is in dialect that drop 'ne' and 'pos', then the frequency of 'ne' and 'pos' will be smaller than in the written corpus. Furthermore, there are phrases using 'ne' but not 'pos', so we need to see the 'ne' and 'pos' as a whole, if one's frequency largely lower than the another, we can still sure that the corpus is in spoken instead of written. However, there are also some drawbacks of doing such thing because we have no information on the size of the corpus we are having, if a small corpus, we will not be that confident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadacbb",
   "metadata": {},
   "source": [
    "* In English, \"weasel words\" are terms or phrases that make statements appear stronger, more authoritative, or more precise than they actually are, without committing to a specific claim. Examples include quantifiers like \"many\" or \"some\", vague sources like \"experts say\", or hedging expressions like \"it is believed that\". Suppose you have a large corpus of text that you're using to train an AI chatbot - you want to remove any sentences that use weasel words, because they are misleading statements. However, you don't want to remove sentences that use the words legitimately (\"Vancouver has many days of rain\" is a truthful statement). How might you go about building a filter that identifies weasel word constructions (assume you can't just use a list of weasel words - that wouldn't work on its own, anyway)? List any assumptions and tools you might need to identify them.  Also explain how you would ensure you avoid false positives.\n",
    "\n",
    "Ans: \n",
    "To do this task, the general assumption is that I can understand the language properly.\n",
    "\n",
    "Firstly, I need to extract vague sentences that include one or more following elements: vague quantifiers such as many, some, or several; vague source such as experts, scholars, scientists; Uncertain phrase such as `it is said that`, `it is believed that`, `studies show that`. To do this task, I need to divide sentences into list of words, each list represent a sentence. Furthermore, I need POS tagger to identify the word function so I can narrow down the size.\n",
    "\n",
    "Secondly, since we don't have any label on the target, we can not use any machine learning model to learn the correlations between words and targets right now. So mannualy classification is important, by setting if-else statments, we can label the sentence with positive, negative, and can not decide. For classify the Positives, we can check if the sentence contain any vague source phrases, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2ddfb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
